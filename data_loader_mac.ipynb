{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "# Data Loader for Mac with Google Drive\n",
    "\n",
    "Install Google Drive Desktop app and sync your folders, then update the paths below to match your Google Drive location.\n",
    "\n",
    "Typical Mac Google Drive path: `~/Library/CloudStorage/GoogleDrive-<your-email>/My Drive/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_paths",
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pathlib import Path\n\n# UPDATE THIS: Your Google Drive path on Mac\nGOOGLE_DRIVE_ROOT = Path.home() / \"Library/CloudStorage/GoogleDrive-jokas.jasas@gmail.com/My Drive\"\n\n# Data paths (adjust folder names as needed)\ndata_path = GOOGLE_DRIVE_ROOT / \"dc4data\"\nbenthic_path = data_path / \"benthic_datasets\"\ncoralbleaching_path = data_path / \"coral_bleaching\"\ncoralscapes_path = data_path / \"coralscapes\"\n\n# HuggingFace cache directory (use Google Drive to save disk space)\nHF_CACHE_DIR = GOOGLE_DRIVE_ROOT / \".hf_cache\"\nHF_CACHE_DIR.mkdir(exist_ok=True)\n\nprint(f\"HuggingFace cache: {HF_CACHE_DIR}\")\n\n# Verify paths exist\nfor p in [data_path, benthic_path, coralbleaching_path, coralscapes_path]:\n    if os.path.exists(p):\n        print(f\"✓ Path exists: {p}\")\n    else:\n        print(f\"✗ Path NOT found: {p}\")\n        print(f\"  Please check if folder is synced in Google Drive Desktop\")"
  },
  {
   "cell_type": "markdown",
   "id": "benthic_header",
   "metadata": {},
   "source": [
    "## Benthic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "benthic_dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 8 benthic datasets\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Update these paths to match your Google Drive structure\n",
    "benthic_base = benthic_path / \"mask_labels/reef_support\"\n",
    "\n",
    "benthic_folders = [\n",
    "    \"SEAFLOWER_BOLIVAR\",\n",
    "    \"SEAFLOWER_COURTOWN\",\n",
    "    \"SEAVIEW_PAC_USA\",\n",
    "    \"SEAVIEW_IDN_PHL\",\n",
    "    \"SEAVIEW_PAC_AUS\",\n",
    "    \"TETES_PROVIDENCIA\",\n",
    "    \"SEAVIEW_ATL\",\n",
    "    \"UNAL_BLEACHING_TAYRONA\",\n",
    "]\n",
    "\n",
    "benthic_paths = [benthic_base / folder for folder in benthic_folders]\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # keep only typical image files; sorted for reproducibility\n",
    "        exts = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\")\n",
    "        self.images = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(exts)])\n",
    "\n",
    "        if not self.images:\n",
    "            raise FileNotFoundError(f\"No images found in {img_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "        # Map \"name.ext\" -> \"name_mask.png\"\n",
    "        stem = Path(img_name).stem\n",
    "        mask_name = f\"{stem}_mask.png\"\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "\n",
    "        if not os.path.exists(mask_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Mask not found for {img_name}. Expected: {mask_path} \"\n",
    "                \"(pattern '<image_stem>_mask.png').\"\n",
    "            )\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "def get_mask(benthic_folder):\n",
    "    mask_path = os.path.join(benthic_folder, 'masks_stitched')\n",
    "    return mask_path\n",
    "\n",
    "def get_image(benthic_folder):\n",
    "    image_path = os.path.join(benthic_folder, 'images')\n",
    "    return image_path\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "SEAFLOWER_BOLIVAR = SegmentationDataset(get_image(benthic_paths[0]), get_mask(benthic_paths[0]))\n",
    "SEAFLOWER_COURTOWN = SegmentationDataset(get_image(benthic_paths[1]), get_mask(benthic_paths[1]))\n",
    "SEAVIEW_PAC_USA = SegmentationDataset(get_image(benthic_paths[2]), get_mask(benthic_paths[2]))\n",
    "SEAVIEW_IDN_PHL = SegmentationDataset(get_image(benthic_paths[3]), get_mask(benthic_paths[3]))\n",
    "SEAVIEW_PAC_AUS = SegmentationDataset(get_image(benthic_paths[4]), get_mask(benthic_paths[4]))\n",
    "TETES_PROVIDENCIA = SegmentationDataset(get_image(benthic_paths[5]), get_mask(benthic_paths[5]))\n",
    "SEAVIEW_ATL = SegmentationDataset(get_image(benthic_paths[6]), get_mask(benthic_paths[6]))\n",
    "UNAL_BLEACHING_TAYRONA = SegmentationDataset(get_image(benthic_paths[7]), get_mask(benthic_paths[7]))\n",
    "\n",
    "print(f\"✓ Loaded {len(benthic_folders)} benthic datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coralscapes_header",
   "metadata": {},
   "source": [
    "## Coral Scapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coralscapes_dataset",
   "metadata": {},
   "outputs": [],
   "source": "from datasets import Dataset as HFDataset, concatenate_datasets\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport pyarrow.parquet as pq\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import Optional, Callable, Union, Sequence\n\nclass _ParquetMasksByIndex:\n    def __init__(self, parquet_dir_or_paths: Union[str, Path, Sequence[Union[str, Path]]],\n                 column_png: str = \"label_health_rgb_png\"):\n        # normalize to list of parquet files\n        if isinstance(parquet_dir_or_paths, (str, Path)):\n            p = Path(parquet_dir_or_paths)\n            if p.is_dir():\n                paths = sorted(p.glob(\"*.parquet\"))\n                if not paths:\n                    raise FileNotFoundError(f\"No parquet files in directory: {p}\")\n            else:\n                if not p.exists():\n                    raise FileNotFoundError(f\"Parquet file not found: {p}\")\n                paths = [p]\n        else:\n            paths = [Path(x) for x in parquet_dir_or_paths]\n            for p in paths:\n                if not p.exists():\n                    raise FileNotFoundError(f\"Parquet file not found: {p}\")\n\n        self._tables = [pq.read_table(p) for p in paths]\n        for t in self._tables:\n            if \"index\" not in t.column_names or column_png not in t.column_names:\n                raise ValueError(f\"Parquet must have 'index' and '{column_png}'. Got: {t.column_names}\")\n        self._colname = column_png\n\n        # build index -> (table_id, row_id)\n        self._map = {}\n        for tid, t in enumerate(self._tables):\n            idxs = t[\"index\"].to_pylist()\n            for rid, ds_idx in enumerate(idxs):\n                self._map[int(ds_idx)] = (tid, rid)\n\n    def get_mask_pil(self, ds_index: int) -> Image.Image:\n        tid, rid = self._map[ds_index]\n        cell = self._tables[tid][self._colname][rid].as_py()\n        if isinstance(cell, memoryview):\n            cell = cell.tobytes()\n        elif isinstance(cell, bytearray):\n            cell = bytes(cell)\n        return Image.open(BytesIO(cell)).convert(\"RGB\")\n    \nclass CoralScapesImagesMasks(Dataset):\n    \"\"\"\n    Load CoralScapes from local Arrow files (no HuggingFace download needed!)\n    \n    Images: From local Arrow files on Google Drive\n    Masks: From local Parquet files\n    \"\"\"\n    def __init__(self,\n                 parquet_dir_or_paths: Union[str, Path, Sequence[Union[str, Path]]],\n                 arrow_dir: Union[str, Path],\n                 img_transform: Optional[Callable] = None,\n                 mask_transform: Optional[Callable] = None):\n        \n        # Load images from local Arrow files\n        arrow_path = Path(arrow_dir)\n        if not arrow_path.exists():\n            raise FileNotFoundError(f\"Arrow directory not found: {arrow_path}\")\n        \n        arrow_files = sorted(arrow_path.glob(\"*.arrow\"))\n        if not arrow_files:\n            raise FileNotFoundError(f\"No arrow files found in: {arrow_path}\")\n        \n        print(f\"Loading from {len(arrow_files)} Arrow files in {arrow_path}\")\n        shards = [HFDataset.from_file(str(p)) for p in arrow_files]\n        self.img_ds = concatenate_datasets(shards) if len(shards) > 1 else shards[0]\n        print(f\"✓ Loaded {len(self.img_ds)} images\")\n\n        # Masks from parquet\n        self.masks = _ParquetMasksByIndex(parquet_dir_or_paths)\n\n        self.img_tf = img_transform\n        self.mask_tf = mask_transform\n\n    def __len__(self):\n        return len(self.img_ds)\n\n    def __getitem__(self, idx: int):\n        rec = self.img_ds[idx]\n        img: Image.Image = rec[\"image\"].convert(\"RGB\")\n        mask: Image.Image = self.masks.get_mask_pil(idx)\n\n        if self.img_tf is not None:\n            img = self.img_tf(img)\n        if self.mask_tf is not None:\n            mask = self.mask_tf(mask)\n        return img, mask\n\n\n# Paths to LOCAL data (no downloads needed!)\nTRAIN_PARQUET_DIR = \"data_preprocessing/coralscapes_export/parquet/train\"\nVAL_PARQUET_DIR   = \"data_preprocessing/coralscapes_export/parquet/validation\"\nTEST_PARQUET_DIR  = \"data_preprocessing/coralscapes_export/parquet/test\"\n\nTRAIN_ARROW_DIR = coralscapes_path / \"train\"\nVAL_ARROW_DIR = coralscapes_path / \"validation\"\nTEST_ARROW_DIR = coralscapes_path / \"test\"\n\n# Load from local files - NO HUGGINGFACE DOWNLOAD!\nCORALSCAPES_train = CoralScapesImagesMasks(parquet_dir_or_paths=TRAIN_PARQUET_DIR, arrow_dir=TRAIN_ARROW_DIR)\nCORALSCAPES_val   = CoralScapesImagesMasks(parquet_dir_or_paths=VAL_PARQUET_DIR, arrow_dir=VAL_ARROW_DIR)\nCORALSCAPES_test  = CoralScapesImagesMasks(parquet_dir_or_paths=TEST_PARQUET_DIR, arrow_dir=TEST_ARROW_DIR)\n\nprint(f\"✓ All CoralScapes datasets loaded from local files!\")"
  },
  {
   "cell_type": "markdown",
   "id": "bleaching_header",
   "metadata": {},
   "source": [
    "## Coral Bleaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bleaching_paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE: Point to your Google Drive coral bleaching data\n",
    "coral_bleaching_images = coralbleaching_path / \"reef_support/UNAL_BLEACHING_TAYRONA/images\"\n",
    "coral_bleaching_combined_masks = \"data_preprocessing/coralbleaching/combined_masks\"\n",
    "coral_bleaching_single_masks = \"data_preprocessing/coralbleaching/single_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bleaching_dataset",
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom torchvision.transforms import Resize\n\nresize = Resize((640, 640))\n\ndef pil_to_tensor(img):\n    \"\"\"Converts PIL image to normalized torch tensor and resizes to 640x640.\"\"\"\n    if img is None:\n        raise ValueError(\"Received None instead of a PIL.Image.\")\n    if isinstance(img, torch.Tensor):\n        return img\n    img = resize(img)\n    a = np.asarray(img.convert(\"RGB\"), dtype=np.uint8).copy()\n    return torch.from_numpy(a).permute(2, 0, 1).float() / 255.0\n\n\nclass CoralBleachingDataset(Dataset):\n    def __init__(self, images_dir, combined_dir, single_dir):\n        self.images_dir = Path(images_dir)\n        self.combined_dir = Path(combined_dir)\n        self.single_bleached = Path(single_dir) / \"bleached_blue\"\n        self.single_non = Path(single_dir) / \"non_bleached_red\"\n\n        imgs = []\n        for e in (\"*.png\",\"*.jpg\",\"*.jpeg\"):\n            imgs += list(self.images_dir.glob(e))\n        self.images = sorted(imgs)\n\n        self.pairs = self._match_pairs()\n        print(f\"Found {len(self.pairs)} image-mask pairs\")\n\n    def _match_pairs(self):\n        def index_dir(d):\n            out={}\n            for e in (\"*.png\",\"*.jpg\",\"*.jpeg\"):\n                for p in d.glob(e): \n                    out[p.stem.lower()] = p\n            return out\n        \n        cmb = index_dir(self.combined_dir)\n        ble = index_dir(self.single_bleached) if self.single_bleached.exists() else {}\n        non = index_dir(self.single_non) if self.single_non.exists() else {}\n\n        pairs=[]\n        for img in self.images:\n            # Remove _corr suffix if present\n            key = img.stem.lower().replace(\"_corr\", \"\")\n            \n            # Try combined mask first\n            k_cmb = f\"{key}_combined\"\n            if k_cmb in cmb: \n                pairs.append((img, cmb[k_cmb]))\n                continue\n            \n            # Try bleached mask\n            cand = [p for k,p in ble.items() if key in k or k in key]\n            if cand: \n                pairs.append((img, cand[0]))\n                continue\n            \n            # Try non-bleached mask\n            cand = [p for k,p in non.items() if key in k or k in key]\n            if cand: \n                pairs.append((img, cand[0]))\n                \n        return pairs\n\n    def __len__(self): \n        return len(self.pairs)\n    \n    def __getitem__(self, i):\n        ip, mp = self.pairs[i]\n        x = pil_to_tensor(Image.open(ip))\n        y = pil_to_tensor(Image.open(mp))\n        return x, y\n\ndef pad_collate(batch):\n    imgs, masks = zip(*batch)\n    C = imgs[0].shape[0]\n    H = max(t.shape[1] for t in imgs)\n    W = max(t.shape[2] for t in imgs)\n    xb = torch.zeros(len(imgs), C, H, W, dtype=imgs[0].dtype)\n    yb = torch.zeros(len(masks), C, H, W, dtype=masks[0].dtype)\n    for i, (x, y) in enumerate(zip(imgs, masks)):\n        h, w = x.shape[1], x.shape[2]\n        xb[i, :, :h, :w] = x\n        yb[i, :, :h, :w] = y\n    return xb, yb\n\ndataset = CoralBleachingDataset(\n    images_dir=coral_bleaching_images,\n    combined_dir=coral_bleaching_combined_masks,\n    single_dir=coral_bleaching_single_masks\n)\n\nif len(dataset) > 0:\n    loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0, collate_fn=pad_collate)\n    xb, yb = next(iter(loader))\n    print(f\"✓ Coral Bleaching dataset: {xb.shape}, {yb.shape}\")\nelse:\n    print(\"⚠ No coral bleaching pairs found - skipping this dataset\")"
  },
  {
   "cell_type": "markdown",
   "id": "combine_header",
   "metadata": {},
   "source": [
    "# Combine ALL into ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combine_all",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
    "\n",
    "def pil_to_tensor_rgb(img):\n",
    "    if img is None:\n",
    "        raise ValueError(\"Received None instead of a PIL.Image.\")\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        return img\n",
    "    a = np.asarray(img.convert(\"RGB\"), dtype=np.uint8)\n",
    "    return torch.from_numpy(a).permute(2,0,1).float() / 255.0\n",
    "\n",
    "\n",
    "class ToTensorPair:\n",
    "    def __call__(self, img: Image.Image, mask: Image.Image):\n",
    "        return pil_to_tensor_rgb(img), pil_to_tensor_rgb(mask)\n",
    "\n",
    "\n",
    "class PairTransformWrapper(Dataset):\n",
    "    def __init__(self, base_ds: Dataset, img_tf=None, mask_tf=None):\n",
    "        self.base = base_ds\n",
    "        self.img_tf = img_tf\n",
    "        self.mask_tf = mask_tf\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img, mask = self.base[idx]\n",
    "        if self.img_tf is not None:\n",
    "            img = self.img_tf(img)\n",
    "        if self.mask_tf is not None:\n",
    "            mask = self.mask_tf(mask)\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "# Wrap benthic datasets\n",
    "BOLIVAR_t     = PairTransformWrapper(SEAFLOWER_BOLIVAR,     img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "COURTOWN_t    = PairTransformWrapper(SEAFLOWER_COURTOWN,    img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "PAC_USA_t     = PairTransformWrapper(SEAVIEW_PAC_USA,       img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "IDN_PHL_t     = PairTransformWrapper(SEAVIEW_IDN_PHL,       img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "PAC_AUS_t     = PairTransformWrapper(SEAVIEW_PAC_AUS,       img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "TETES_t       = PairTransformWrapper(TETES_PROVIDENCIA,     img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "ATL_t         = PairTransformWrapper(SEAVIEW_ATL,           img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "TAYRONA_t     = PairTransformWrapper(UNAL_BLEACHING_TAYRONA, img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "\n",
    "# Wrap CoralScapes\n",
    "CS_train_t    = PairTransformWrapper(CORALSCAPES_train,     img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "CS_val_t      = PairTransformWrapper(CORALSCAPES_val,       img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "CS_test_t     = PairTransformWrapper(CORALSCAPES_test,      img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "\n",
    "# Bleaching dataset already returns tensors\n",
    "BLEACH_all_t  = dataset\n",
    "\n",
    "# Concatenate all datasets\n",
    "ALL_DATA = ConcatDataset([\n",
    "    BOLIVAR_t, COURTOWN_t, PAC_USA_t, IDN_PHL_t, PAC_AUS_t, TETES_t, ATL_t, TAYRONA_t,\n",
    "    CS_train_t, CS_val_t, CS_test_t,\n",
    "    BLEACH_all_t,\n",
    "])\n",
    "\n",
    "loader_all = DataLoader(\n",
    "    ALL_DATA,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=pad_collate,\n",
    ")\n",
    "\n",
    "xb, yb = next(iter(loader_all))\n",
    "print(f\"✓ Combined dataset: {xb.shape}, {yb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yolo_header",
   "metadata": {},
   "source": [
    "# YOLO Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yolo_wrapper",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CoralTrainWrapper(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps the combined ALL_DATA dataset to produce:\n",
    "    image: (3,H,W)\n",
    "    mask: (1,H,W)\n",
    "    label: scalar (0=healthy, 1=bleached)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ds):\n",
    "        self.base = base_ds\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, mask = self.base[idx]\n",
    "        img = img.float()\n",
    "        mask = mask.float()\n",
    "\n",
    "        # Create binary coral presence mask (1 = coral area, 0 = background)\n",
    "        coral_mask = (mask.sum(dim=0, keepdim=True) > 0).float()\n",
    "\n",
    "        # Determine bleaching label from color mask (red>blue → bleached)\n",
    "        red_pixels = mask[0] > mask[2]\n",
    "        bleached = (red_pixels.float().mean() > 0.01)\n",
    "        label = torch.tensor(int(bleached), dtype=torch.long)\n",
    "\n",
    "        return {\"image\": img, \"mask\": coral_mask, \"label\": label}\n",
    "\n",
    "wrapped_data = CoralTrainWrapper(ALL_DATA)\n",
    "\n",
    "loader_all = DataLoader(\n",
    "    wrapped_data,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"✓ YOLO wrapper ready with {len(wrapped_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yolo_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from coral_yolo.models.coral_classifier import CoralClassifier\n",
    "from coral_yolo.losses.classification_loss import CoralClassificationLoss\n",
    "from coral_yolo.engine.metrics import ClsPRF1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = CoralClassifier(\n",
    "    yolo_weights=\"yolo11s.pt\",\n",
    "    num_classes=2,\n",
    "    freeze_backbone=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = CoralClassificationLoss()\n",
    "metric = ClsPRF1()\n",
    "\n",
    "print(\"✓ Model initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "yolo_train",
   "metadata": {},
   "outputs": [],
   "source": "# ResNet Coral Health Classification (3 Classes)"
  },
  {
   "cell_type": "code",
   "id": "1ecodo0985c",
   "source": "import torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nimport numpy as np\n\nclass CoralHealthDataset(Dataset):\n    \"\"\"\n    2-class coral health classification dataset.\n    \n    Classes:\n        0: Healthy (red channel high in mask)\n        1: Unhealthy (blue channel high in mask)\n    \"\"\"\n    def __init__(self, base_ds, transform=None):\n        self.base = base_ds\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n    def __len__(self):\n        return len(self.base)\n\n    def __getitem__(self, idx):\n        img, mask = self.base[idx]\n        \n        # img is already (3, H, W) tensor normalized to [0, 1]\n        # mask is (3, H, W) RGB tensor\n        \n        # Apply transforms\n        img = self.transform(img)\n        \n        # Determine class from mask colors\n        # Red channel high = healthy (class 0)\n        # Blue channel high = unhealthy (class 1)\n        \n        red_mean = mask[0].mean().item()\n        blue_mean = mask[2].mean().item()\n        \n        # Classification logic\n        if red_mean > blue_mean:\n            label = 0  # Healthy (red)\n        else:\n            label = 1  # Unhealthy (blue)\n        \n        return img, label\n\n\n# Create ResNet dataset from combined data\nresnet_dataset = CoralHealthDataset(ALL_DATA)\n\n# Train/val split (80/20) - same split as YOLO\ntrain_size = int(0.8 * len(resnet_dataset))\nval_size = len(resnet_dataset) - train_size\nresnet_train, resnet_val = random_split(resnet_dataset, [train_size, val_size])\n\n# Create dataloaders\nresnet_train_loader = DataLoader(resnet_train, batch_size=32, shuffle=True, num_workers=0)\nresnet_val_loader = DataLoader(resnet_val, batch_size=32, shuffle=False, num_workers=0)\n\nprint(f\"✓ ResNet dataset prepared:\")\nprint(f\"  Train samples: {len(resnet_train)}\")\nprint(f\"  Val samples: {len(resnet_val)}\")\nprint(f\"  Classes: 0=Healthy, 1=Unhealthy\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "vfdas176tch",
   "source": "import sys\nsys.path.append('/Users/jokubas/Desktop/Coral-reefs-DBL4')\n\nfrom resnet import CoralResNet, CoralTrainer\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Initialize model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nresnet_model = CoralResNet(\n    num_classes=2,\n    pretrained=True,\n    freeze_backbone=True\n)\n\n# Setup training\noptimizer = optim.AdamW(resnet_model.parameters(), lr=1e-3, weight_decay=1e-4)\ncriterion = nn.CrossEntropyLoss()\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n\n# Initialize trainer\ntrainer = CoralTrainer(\n    model=resnet_model,\n    device=device,\n    class_names=[\"Healthy\", \"Unhealthy\"]\n)\n\nprint(\"✓ ResNet model initialized\")\nprint(f\"  Total parameters: {sum(p.numel() for p in resnet_model.parameters()):,}\")\nprint(f\"  Trainable parameters: {sum(p.numel() for p in resnet_model.parameters() if p.requires_grad):,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4784mm4krvm",
   "source": "# Train ResNet model\nprint(\"Starting ResNet training...\")\nprint(\"=\"*60)\n\nhistory = trainer.fit(\n    train_loader=resnet_train_loader,\n    val_loader=resnet_val_loader,\n    epochs=10,\n    optimizer=optimizer,\n    criterion=criterion,\n    scheduler=scheduler\n)\n\nprint(\"\\n✓ Training complete!\")\nprint(f\"Best model saved to: resnet/best_model.pth\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dc2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}