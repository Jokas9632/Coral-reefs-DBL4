{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a4c6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path exists: G:\\.shortcut-targets-by-id\\1v4g4qOrbisBvrpqOxLrYn96nd_gPG_Ge\\dc4data\n",
      "Path exists: G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\n",
      "Path exists: G:\\.shortcut-targets-by-id\\1jGkNA1n0znoxKnQBHTJZuPgvkiu_OBM8\\coral_bleaching\n",
      "Path exists: G:\\.shortcut-targets-by-id\\1v4g4qOrbisBvrpqOxLrYn96nd_gPG_Ge\\dc4data\\coralscapes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import win32com.client\n",
    "def resolve_shortcut(path):\n",
    "    shell = win32com.client.Dispatch(\"WScript.Shell\")\n",
    "    shortcut = shell.CreateShortCut(path)\n",
    "    return shortcut.Targetpath\n",
    "\n",
    "data_path = resolve_shortcut(r\"G:\\\\My Drive\\\\dc4data.lnk\")\n",
    "benthic_path = resolve_shortcut(data_path+r\"\\\\benthic_datasets.lnk\")\n",
    "coralbleaching_path = resolve_shortcut(data_path+r\"\\\\coral_bleaching.lnk\")\n",
    "if not os.path.exists(r\"G:\\.shortcut-targets-by-id\\1v4g4qOrbisBvrpqOxLrYn96nd_gPG_Ge\\dc4data\\coralscapes\"):\n",
    "     coralscapes_path = resolve_shortcut(data_path+r\"\\\\coralscapes.lnk\")\n",
    "else:\n",
    "        coralscapes_path = r\"G:\\.shortcut-targets-by-id\\1v4g4qOrbisBvrpqOxLrYn96nd_gPG_Ge\\dc4data\\coralscapes\"\n",
    "for p in [data_path, benthic_path, coralbleaching_path, coralscapes_path]:\n",
    "    if os.path.exists(p):\n",
    "        print(f\"Path exists: {p}\")\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Path does not exist: {p}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494b98d",
   "metadata": {},
   "source": [
    "## Benthic Datset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203fed2",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b408d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "benthic_paths = [r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAFLOWER_BOLIVAR\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAFLOWER_COURTOWN\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAVIEW_PAC_USA\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAVIEW_IDN_PHL\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAVIEW_PAC_AUS\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\TETES_PROVIDENCIA\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAVIEW_ATL\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\UNAL_BLEACHING_TAYRONA\",]\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.images = os.listdir(img_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "def get_mask(benthic_folder):\n",
    "    mask_path = os.path.join(benthic_folder, 'masks')\n",
    "    return mask_path\n",
    "\n",
    "def get_image(benthic_folder):\n",
    "    image_path = os.path.join(benthic_folder, 'images')\n",
    "    return image_path\n",
    "\n",
    "\n",
    "#DATASETS\n",
    "SEAFLOWER_BOLIVAR = SegmentationDataset(get_image(benthic_paths[0]), get_mask(benthic_paths[0]))\n",
    "SEAFLOWER_COURTOWN = SegmentationDataset(get_image(benthic_paths[1]), get_mask(benthic_paths[1]))\n",
    "SEAVIEW_PAC_USA = SegmentationDataset(get_image(benthic_paths[2]), get_mask(benthic_paths[2]))\n",
    "SEAVIEW_IDN_PHL = SegmentationDataset(get_image(benthic_paths[3]), get_mask(benthic_paths[3]))\n",
    "SEAVIEW_PAC_AUS = SegmentationDataset(get_image(benthic_paths[4]), get_mask(benthic_paths[4]))\n",
    "TETES_PROVIDENCIA = SegmentationDataset(get_image(benthic_paths[5]), get_mask(benthic_paths[5]))\n",
    "SEAVIEW_ATL = SegmentationDataset(get_image(benthic_paths[6]), get_mask(benthic_paths[6]))\n",
    "UNAL_BLEACHING_TAYRONA = SegmentationDataset(get_image(benthic_paths[7]), get_mask(benthic_paths[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427dad19",
   "metadata": {},
   "source": [
    "## Coral Scapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c8eeb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No parquet files in data_preprocessing\\coralscapes_export_with_images\\parquet\\test",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     27\u001b[39m         \u001b[38;5;28mself\u001b[39m.return_masks_as_long = return_masks_as_long\n\u001b[32m     30\u001b[39m root = Path(\u001b[33m\"\u001b[39m\u001b[33mdata_preprocessing/coralscapes_export_with_images/parquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m coral_scapes_datasets = {s: \u001b[43mCoralScapesParquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_masks_as_long\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m)}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mCoralScapesParquet.__init__\u001b[39m\u001b[34m(self, parquet_dir, return_masks_as_long, cache_files)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mself\u001b[39m.paths = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m.dir.glob(\u001b[33m\"\u001b[39m\u001b[33m*.parquet\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.paths:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo parquet files in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Build global index: which file and which row\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mself\u001b[39m.file_row_offsets = []  \u001b[38;5;66;03m# cumulative row counts\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No parquet files in data_preprocessing\\coralscapes_export_with_images\\parquet\\test"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "class CoralScapesParquet(Dataset):\n",
    "    \"\"\"\n",
    "    Reads shards produced by your pipeline:\n",
    "      columns: split (str), index (int), image_png (bytes), label_health_rgb_png (bytes)\n",
    "    Returns: (C,H,W) float tensors in [0,1] for image and mask.\n",
    "    \"\"\"\n",
    "    def __init__(self, parquet_dir: str | Path, return_masks_as_long: bool = False, cache_files: int = 2):\n",
    "        self.dir = Path(parquet_dir)\n",
    "        self.paths = sorted(self.dir.glob(\"*.parquet\"))\n",
    "        if not self.paths:\n",
    "            raise FileNotFoundError(f\"No parquet files in {self.dir}\")\n",
    "        # Build global index: which file and which row\n",
    "        self.file_row_offsets = []  # cumulative row counts\n",
    "        self.row_index = []         # list of (file_idx, row_idx)\n",
    "        self._meta_row_counts = []\n",
    "        for fidx, p in enumerate(self.paths):\n",
    "            pf = pq.ParquetFile(p)\n",
    "            nrows = pf.metadata.num_rows\n",
    "            self._meta_row_counts.append(nrows)\n",
    "            self.row_index.extend([(fidx, r) for r in range(nrows)])\n",
    "        # Simple LRU cache for loaded tables\n",
    "        self._cache = OrderedDict()\n",
    "        self._cache_limit = max(1, int(cache_files))\n",
    "        self.return_masks_as_long = return_masks_as_long\n",
    "\n",
    "\n",
    "root = Path(\"data_preprocessing/coralscapes_export_with_images/parquet\")\n",
    "coral_scapes_datasets = {s: CoralScapesParquet(root / s, return_masks_as_long=True) for s in (\"test\", \"train\", \"validation\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34905e8c",
   "metadata": {},
   "source": [
    "## Coral Bleaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3dd5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_bleaching_images = r\"g:\\.shortcut-targets-by-id\\1jGkNA1n0znoxKnQBHTJZuPgvkiu_OBM8\\coral_bleaching\\reef_support\\UNAL_BLEACHING_TAYRONA\\images\"\n",
    "coral_bleaching_combined_masks = r\"data_preprocessing/coralbleaching/combined_masks\"\n",
    "coral_bleaching_single_masks = r\"data_preprocessing/coralbleaching/single_masks\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7451e285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayan\\AppData\\Local\\Temp\\ipykernel_45332\\2167386991.py:9: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:209.)\n",
      "  return torch.from_numpy(a).permute(2,0,1).float()/255.0  # (3,H,W)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 3217, 4301]) torch.Size([8, 3, 3217, 4301])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "def pil_to_tensor(img):\n",
    "    a = np.asarray(img.convert(\"RGB\"), dtype=np.uint8)  # (H,W,3)\n",
    "    return torch.from_numpy(a).permute(2,0,1).float()/255.0  # (3,H,W)\n",
    "\n",
    "class CoralBleachingDataset(Dataset):\n",
    "    def __init__(self, images_dir, combined_dir, single_dir):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.combined_dir = Path(combined_dir)\n",
    "        self.single_bleached = Path(single_dir) / \"bleached_blue\"\n",
    "        self.single_non = Path(single_dir) / \"non_bleached_red\"\n",
    "\n",
    "        imgs = []\n",
    "        for e in (\"*.png\",\"*.jpg\",\"*.jpeg\"):\n",
    "            imgs += list(self.images_dir.glob(e))\n",
    "        self.images = sorted(imgs)\n",
    "\n",
    "        self.pairs = self._match_pairs()\n",
    "\n",
    "    def _match_pairs(self):\n",
    "        def index_dir(d):\n",
    "            out={}\n",
    "            for e in (\"*.png\",\"*.jpg\",\"*.jpeg\"):\n",
    "                for p in d.glob(e): out[p.stem.lower()] = p\n",
    "            return out\n",
    "        cmb = index_dir(self.combined_dir)\n",
    "        ble = index_dir(self.single_bleached)\n",
    "        non = index_dir(self.single_non)\n",
    "\n",
    "        pairs=[]\n",
    "        for img in self.images:\n",
    "            key = img.stem.lower()\n",
    "            k_cmb = f\"{key}_combined\"\n",
    "            if k_cmb in cmb: pairs.append((img, cmb[k_cmb])); continue\n",
    "            cand = [p for k,p in ble.items() if k.startswith(key) or key in k]\n",
    "            if cand: pairs.append((img, cand[0])); continue\n",
    "            cand = [p for k,p in non.items() if k.startswith(key) or key in k]\n",
    "            if cand: pairs.append((img, cand[0]))\n",
    "        return pairs\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, i):\n",
    "        ip, mp = self.pairs[i]\n",
    "        x = pil_to_tensor(Image.open(ip))\n",
    "        y = pil_to_tensor(Image.open(mp))\n",
    "        return x, y  # (3,H,W), (3,H,W)\n",
    "\n",
    "def pad_collate(batch):\n",
    "    # batch: list of (img, mask) with varying H,W\n",
    "    imgs, masks = zip(*batch)\n",
    "    C = imgs[0].shape[0]\n",
    "    H = max(t.shape[1] for t in imgs)\n",
    "    W = max(t.shape[2] for t in imgs)\n",
    "    xb = torch.zeros(len(imgs), C, H, W, dtype=imgs[0].dtype)\n",
    "    yb = torch.zeros(len(masks), C, H, W, dtype=masks[0].dtype)\n",
    "    for i, (x, y) in enumerate(zip(imgs, masks)):\n",
    "        h, w = x.shape[1], x.shape[2]\n",
    "        xb[i, :, :h, :w] = x\n",
    "        yb[i, :, :h, :w] = y\n",
    "    return xb, yb\n",
    "\n",
    "# ---- use it ----\n",
    "dataset = CoralBleachingDataset(\n",
    "    images_dir=r\"g:\\.shortcut-targets-by-id\\1jGkNA1n0znoxKnQBHTJZuPgvkiu_OBM8\\coral_bleaching\\reef_support\\UNAL_BLEACHING_TAYRONA\\images\",\n",
    "    combined_dir=r\"data_preprocessing/coralbleaching/combined_masks\",\n",
    "    single_dir=r\"data_preprocessing/coralbleaching/single_masks\"\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0, collate_fn=pad_collate)\n",
    "\n",
    "xb, yb = next(iter(loader))\n",
    "print(xb.shape, yb.shape)  # -> (B,3,H_max,W_max) (B,3,H_max,W_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e17d3b1-3b29-4725-9c50-c233b34eb8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7617a59f-af5a-4250-9738-572f61e3b0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
