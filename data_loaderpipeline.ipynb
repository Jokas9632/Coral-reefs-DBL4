{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a4c6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path exists: G:\\.shortcut-targets-by-id\\1v4g4qOrbisBvrpqOxLrYn96nd_gPG_Ge\\dc4data\n",
      "Path exists: G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\n",
      "Path exists: G:\\.shortcut-targets-by-id\\1jGkNA1n0znoxKnQBHTJZuPgvkiu_OBM8\\coral_bleaching\n",
      "Path exists: G:\\.shortcut-targets-by-id\\1v4g4qOrbisBvrpqOxLrYn96nd_gPG_Ge\\dc4data\\coralscapes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import win32com.client\n",
    "def resolve_shortcut(path):\n",
    "    shell = win32com.client.Dispatch(\"WScript.Shell\")\n",
    "    shortcut = shell.CreateShortCut(path)\n",
    "    return shortcut.Targetpath\n",
    "\n",
    "data_path = resolve_shortcut(r\"G:\\\\My Drive\\\\dc4data.lnk\")\n",
    "benthic_path = resolve_shortcut(data_path+r\"\\\\benthic_datasets.lnk\")\n",
    "coralbleaching_path = resolve_shortcut(data_path+r\"\\\\coral_bleaching.lnk\")\n",
    "if not os.path.exists(r\"G:\\.shortcut-targets-by-id\\1v4g4qOrbisBvrpqOxLrYn96nd_gPG_Ge\\dc4data\\coralscapes\"):\n",
    "     coralscapes_path = resolve_shortcut(data_path+r\"\\\\coralscapes.lnk\")\n",
    "else:\n",
    "        coralscapes_path = r\"G:\\.shortcut-targets-by-id\\1v4g4qOrbisBvrpqOxLrYn96nd_gPG_Ge\\dc4data\\coralscapes\"\n",
    "for p in [data_path, benthic_path, coralbleaching_path, coralscapes_path]:\n",
    "    if os.path.exists(p):\n",
    "        print(f\"Path exists: {p}\")\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Path does not exist: {p}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494b98d",
   "metadata": {},
   "source": [
    "## Benthic Datset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203fed2",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b408d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayan\\anaconda3\\envs\\dc4\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, Union, Sequence, List\n",
    "\n",
    "benthic_paths = [r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAFLOWER_BOLIVAR\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAFLOWER_COURTOWN\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAVIEW_PAC_USA\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAVIEW_IDN_PHL\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAVIEW_PAC_AUS\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\TETES_PROVIDENCIA\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAVIEW_ATL\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\UNAL_BLEACHING_TAYRONA\",]\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # keep only typical image files; sorted for reproducibility\n",
    "        exts = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\")\n",
    "        self.images = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(exts)])\n",
    "\n",
    "        if not self.images:\n",
    "            raise FileNotFoundError(f\"No images found in {img_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "        # Map \"name.ext\" -> \"name_mask.png\" (as you requested)\n",
    "        stem = Path(img_name).stem\n",
    "        mask_name = f\"{stem}_mask.png\"\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "\n",
    "        if not os.path.exists(mask_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Mask not found for {img_name}. Expected: {mask_path} \"\n",
    "                \"(pattern '<image_stem>_mask.png').\"\n",
    "            )\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "def get_mask(benthic_folder):\n",
    "    mask_path = os.path.join(benthic_folder, 'masks_stitched')\n",
    "    return mask_path\n",
    "\n",
    "def get_image(benthic_folder):\n",
    "    image_path = os.path.join(benthic_folder, 'images')\n",
    "    return image_path\n",
    "\n",
    "\n",
    "#DATASETS\n",
    "SEAFLOWER_BOLIVAR = SegmentationDataset(get_image(benthic_paths[0]), get_mask(benthic_paths[0]))\n",
    "SEAFLOWER_COURTOWN = SegmentationDataset(get_image(benthic_paths[1]), get_mask(benthic_paths[1]))\n",
    "SEAVIEW_PAC_USA = SegmentationDataset(get_image(benthic_paths[2]), get_mask(benthic_paths[2]))\n",
    "SEAVIEW_IDN_PHL = SegmentationDataset(get_image(benthic_paths[3]), get_mask(benthic_paths[3]))\n",
    "SEAVIEW_PAC_AUS = SegmentationDataset(get_image(benthic_paths[4]), get_mask(benthic_paths[4]))\n",
    "TETES_PROVIDENCIA = SegmentationDataset(get_image(benthic_paths[5]), get_mask(benthic_paths[5]))\n",
    "SEAVIEW_ATL = SegmentationDataset(get_image(benthic_paths[6]), get_mask(benthic_paths[6]))\n",
    "UNAL_BLEACHING_TAYRONA = SegmentationDataset(get_image(benthic_paths[7]), get_mask(benthic_paths[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427dad19",
   "metadata": {},
   "source": [
    "## Coral Scapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c8eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- helper: load masks from parquet parts by dataset index ---\n",
    "class _ParquetMasksByIndex:\n",
    "    def __init__(self, parquet_dir_or_paths: Union[str, Path, Sequence[Union[str, Path]]],\n",
    "                 column_png: str = \"label_health_rgb_png\"):\n",
    "        # normalize to list of parquet files\n",
    "        if isinstance(parquet_dir_or_paths, (str, Path)):\n",
    "            p = Path(parquet_dir_or_paths)\n",
    "            if p.is_dir():\n",
    "                paths = sorted(p.glob(\"*.parquet\"))\n",
    "                if not paths:\n",
    "                    raise FileNotFoundError(f\"No parquet files in directory: {p}\")\n",
    "            else:\n",
    "                if not p.exists():\n",
    "                    raise FileNotFoundError(f\"Parquet file not found: {p}\")\n",
    "                paths = [p]\n",
    "        else:\n",
    "            paths = [Path(x) for x in parquet_dir_or_paths]\n",
    "            for p in paths:\n",
    "                if not p.exists():\n",
    "                    raise FileNotFoundError(f\"Parquet file not found: {p}\")\n",
    "\n",
    "        self._tables = [pq.read_table(p) for p in paths]\n",
    "        for t in self._tables:\n",
    "            if \"index\" not in t.column_names or column_png not in t.column_names:\n",
    "                raise ValueError(f\"Parquet must have 'index' and '{column_png}'. Got: {t.column_names}\")\n",
    "        self._colname = column_png\n",
    "\n",
    "        # build index -> (table_id, row_id)\n",
    "        self._map = {}\n",
    "        for tid, t in enumerate(self._tables):\n",
    "            idxs = t[\"index\"].to_pylist()\n",
    "            for rid, ds_idx in enumerate(idxs):\n",
    "                self._map[int(ds_idx)] = (tid, rid)\n",
    "\n",
    "    def get_mask_pil(self, ds_index: int) -> Image.Image:\n",
    "        tid, rid = self._map[ds_index]\n",
    "        cell = self._tables[tid][self._colname][rid].as_py()\n",
    "        if isinstance(cell, memoryview):\n",
    "            cell = cell.tobytes()\n",
    "        elif isinstance(cell, bytearray):\n",
    "            cell = bytes(cell)\n",
    "        return Image.open(BytesIO(cell)).convert(\"RGB\")\n",
    "    \n",
    "class CoralScapesImagesMasks(Dataset):\n",
    "    \"\"\"\n",
    "    Images:\n",
    "      - Either from HF split (set split=\"train\"/\"validation\"/\"test\")\n",
    "      - Or from local Arrow shards (set arrow_paths=[...])\n",
    "    Masks:\n",
    "      - From your Parquet export (dir or list), matched by dataset index.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 parquet_dir_or_paths: Union[str, Path, Sequence[Union[str, Path]]],\n",
    "                 split: Optional[str] = None,\n",
    "                 arrow_paths: Optional[Sequence[Union[str, Path]]] = None,\n",
    "                 img_transform: Optional[Callable] = None,\n",
    "                 mask_transform: Optional[Callable] = None):\n",
    "        if (split is None) == (arrow_paths is None):\n",
    "            raise ValueError(\"Specify exactly one image source: either `split` (HF) OR `arrow_paths` (local).\")\n",
    "\n",
    "        # Image source\n",
    "        if split is not None:\n",
    "            ds_all = load_dataset(\"EPFL-ECEO/coralscapes\")\n",
    "            if split not in ds_all:\n",
    "                raise ValueError(f\"Split '{split}' not found. Available: {list(ds_all.keys())}\")\n",
    "            self.img_ds: HFDataset = ds_all[split]\n",
    "        else:\n",
    "            paths = [Path(p) for p in arrow_paths]\n",
    "            for p in paths:\n",
    "                if not p.exists():\n",
    "                    raise FileNotFoundError(f\"Arrow file not found: {p}\")\n",
    "            shards = [HFDataset.from_file(p.as_posix()) for p in paths]\n",
    "            self.img_ds = shards[0] if len(shards) == 1 else HFDataset.concatenate_datasets(shards)\n",
    "\n",
    "        # Masks\n",
    "        self.masks = _ParquetMasksByIndex(parquet_dir_or_paths)\n",
    "\n",
    "        self.img_tf = img_transform\n",
    "        self.mask_tf = mask_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ds)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        rec = self.img_ds[idx]\n",
    "        img: Image.Image = rec[\"image\"].convert(\"RGB\")\n",
    "        mask: Image.Image = self.masks.get_mask_pil(idx)  # keyed by dataset index\n",
    "\n",
    "        if self.img_tf is not None:\n",
    "            img = self.img_tf(img)\n",
    "        if self.mask_tf is not None:\n",
    "            mask = self.mask_tf(mask)\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "# --- your instantiation lines (unchanged) ---\n",
    "TRAIN_PARQUET_DIR = r\"data_preprocessing\\coralscapes_export\\parquet\\train\"\n",
    "VAL_PARQUET_DIR   = r\"data_preprocessing\\coralscapes_export\\parquet\\validation\"\n",
    "TEST_PARQUET_DIR  = r\"data_preprocessing\\coralscapes_export\\parquet\\test\"\n",
    "\n",
    "CORALSCAPES_train = CoralScapesImagesMasks(split=\"train\", parquet_dir_or_paths=TRAIN_PARQUET_DIR)\n",
    "CORALSCAPES_val   = CoralScapesImagesMasks(split=\"validation\", parquet_dir_or_paths=VAL_PARQUET_DIR)\n",
    "CORALSCAPES_test  = CoralScapesImagesMasks(split=\"test\", parquet_dir_or_paths=TEST_PARQUET_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34905e8c",
   "metadata": {},
   "source": [
    "## Coral Bleaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3dd5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_bleaching_images = r\"g:\\.shortcut-targets-by-id\\1jGkNA1n0znoxKnQBHTJZuPgvkiu_OBM8\\coral_bleaching\\reef_support\\UNAL_BLEACHING_TAYRONA\\images\"\n",
    "coral_bleaching_combined_masks = r\"data_preprocessing/coralbleaching/combined_masks\"\n",
    "coral_bleaching_single_masks = r\"data_preprocessing/coralbleaching/single_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7451e285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 640, 640]) torch.Size([8, 3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "resize = Resize((640, 640))\n",
    "\n",
    "def pil_to_tensor(img):\n",
    "    \"\"\"Converts PIL image to normalized torch tensor and resizes to 640x640.\"\"\"\n",
    "    if img is None:\n",
    "        raise ValueError(\"Received None instead of a PIL.Image.\")\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        return img  # already a tensor\n",
    "    # --- NEW: resize before converting to tensor ---\n",
    "    img = resize(img)\n",
    "    a = np.asarray(img.convert(\"RGB\"), dtype=np.uint8).copy()\n",
    "    return torch.from_numpy(a).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "\n",
    "class CoralBleachingDataset(Dataset):\n",
    "    def __init__(self, images_dir, combined_dir, single_dir):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.combined_dir = Path(combined_dir)\n",
    "        self.single_bleached = Path(single_dir) / \"bleached_blue\"\n",
    "        self.single_non = Path(single_dir) / \"non_bleached_red\"\n",
    "\n",
    "        imgs = []\n",
    "        for e in (\"*.png\",\"*.jpg\",\"*.jpeg\"):\n",
    "            imgs += list(self.images_dir.glob(e))\n",
    "        self.images = sorted(imgs)\n",
    "\n",
    "        self.pairs = self._match_pairs()\n",
    "\n",
    "    def _match_pairs(self):\n",
    "        def index_dir(d):\n",
    "            out={}\n",
    "            for e in (\"*.png\",\"*.jpg\",\"*.jpeg\"):\n",
    "                for p in d.glob(e): out[p.stem.lower()] = p\n",
    "            return out\n",
    "        cmb = index_dir(self.combined_dir)\n",
    "        ble = index_dir(self.single_bleached)\n",
    "        non = index_dir(self.single_non)\n",
    "\n",
    "        pairs=[]\n",
    "        for img in self.images:\n",
    "            key = img.stem.lower()\n",
    "            k_cmb = f\"{key}_combined\"\n",
    "            if k_cmb in cmb: pairs.append((img, cmb[k_cmb])); continue\n",
    "            cand = [p for k,p in ble.items() if k.startswith(key) or key in k]\n",
    "            if cand: pairs.append((img, cand[0])); continue\n",
    "            cand = [p for k,p in non.items() if k.startswith(key) or key in k]\n",
    "            if cand: pairs.append((img, cand[0]))\n",
    "        return pairs\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, i):\n",
    "        ip, mp = self.pairs[i]\n",
    "        x = pil_to_tensor(Image.open(ip))\n",
    "        y = pil_to_tensor(Image.open(mp))\n",
    "        return x, y  # (3,H,W), (3,H,W)\n",
    "\n",
    "def pad_collate(batch):\n",
    "    # batch: list of (img, mask) with varying H,W\n",
    "    imgs, masks = zip(*batch)\n",
    "    C = imgs[0].shape[0]\n",
    "    H = max(t.shape[1] for t in imgs)\n",
    "    W = max(t.shape[2] for t in imgs)\n",
    "    xb = torch.zeros(len(imgs), C, H, W, dtype=imgs[0].dtype)\n",
    "    yb = torch.zeros(len(masks), C, H, W, dtype=masks[0].dtype)\n",
    "    for i, (x, y) in enumerate(zip(imgs, masks)):\n",
    "        h, w = x.shape[1], x.shape[2]\n",
    "        xb[i, :, :h, :w] = x\n",
    "        yb[i, :, :h, :w] = y\n",
    "    return xb, yb\n",
    "\n",
    "# ---- use it ----\n",
    "dataset = CoralBleachingDataset(\n",
    "    images_dir=r\"g:\\.shortcut-targets-by-id\\1jGkNA1n0znoxKnQBHTJZuPgvkiu_OBM8\\coral_bleaching\\reef_support\\UNAL_BLEACHING_TAYRONA\\images\",\n",
    "    combined_dir=r\"data_preprocessing/coralbleaching/combined_masks\",\n",
    "    single_dir=r\"data_preprocessing/coralbleaching/single_masks\"\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0, collate_fn=pad_collate)\n",
    "\n",
    "xb, yb = next(iter(loader))\n",
    "print(xb.shape, yb.shape)  # -> (B,3,H_max,W_max) (B,3,H_max,W_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8315f",
   "metadata": {},
   "source": [
    "# Combine ALL into ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15160a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayan\\AppData\\Local\\Temp\\ipykernel_13020\\128034685.py:12: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:209.)\n",
      "  return torch.from_numpy(a).permute(2,0,1).float() / 255.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 3000, 4000]) torch.Size([8, 3, 3000, 4000])\n"
     ]
    }
   ],
   "source": [
    "# --- 1) Common PIL->tensor transform for both image and mask ---\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def pil_to_tensor_rgb(img):\n",
    "    if img is None:\n",
    "        raise ValueError(\"Received None instead of a PIL.Image. Check your dataset/__getitem__.\")\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        return img  # already a tensor\n",
    "    a = np.asarray(img.convert(\"RGB\"), dtype=np.uint8)\n",
    "    return torch.from_numpy(a).permute(2,0,1).float() / 255.0\n",
    "\n",
    "\n",
    "class ToTensorPair:\n",
    "    \"\"\"Apply the same PIL->tensor conversion to (image, mask) pairs.\"\"\"\n",
    "    def __call__(self, img: Image.Image, mask: Image.Image) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return pil_to_tensor_rgb(img), pil_to_tensor_rgb(mask)\n",
    "\n",
    "# --- 2) A tiny wrapper to enforce a uniform transform across heterogeneous datasets ---\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PairTransformWrapper(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps any (image, mask) dataset and applies (img_tf, mask_tf) before returning.\n",
    "    If the underlying dataset already returns tensors, you can pass identity lambdas.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ds: Dataset, img_tf=None, mask_tf=None):\n",
    "        self.base = base_ds\n",
    "        self.img_tf = img_tf\n",
    "        self.mask_tf = mask_tf\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img, mask = self.base[idx]\n",
    "        if self.img_tf is not None:\n",
    "            img = self.img_tf(img)\n",
    "        if self.mask_tf is not None:\n",
    "            mask = self.mask_tf(mask)\n",
    "        return img, mask\n",
    "\n",
    "# --- 4) Standardize outputs: wrap PIL-returning datasets so *all* output tensors ---\n",
    "to_tensor = ToTensorPair()\n",
    "\n",
    "# Benthic sets likely return PIL unless you set transform; wrap them:\n",
    "BOLIVAR_t     = PairTransformWrapper(SEAFLOWER_BOLIVAR,     img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "COURTOWN_t    = PairTransformWrapper(SEAFLOWER_COURTOWN,    img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "PAC_USA_t     = PairTransformWrapper(SEAVIEW_PAC_USA,       img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "IDN_PHL_t     = PairTransformWrapper(SEAVIEW_IDN_PHL,       img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "PAC_AUS_t     = PairTransformWrapper(SEAVIEW_PAC_AUS,       img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "TETES_t       = PairTransformWrapper(TETES_PROVIDENCIA,     img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "ATL_t         = PairTransformWrapper(SEAVIEW_ATL,           img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "TAYRONA_t     = PairTransformWrapper(UNAL_BLEACHING_TAYRONA, img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "\n",
    "# Coralscapes (PIL) → wrap as tensors too:\n",
    "CS_train_t    = PairTransformWrapper(CORALSCAPES_train,     img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "CS_val_t      = PairTransformWrapper(CORALSCAPES_val,       img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "CS_test_t     = PairTransformWrapper(CORALSCAPES_test,      img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "\n",
    "# CoralBleachingDataset already returns tensors (3,H,W). If so, no wrap needed:\n",
    "BLEACH_all_t  = dataset  # keep as-is\n",
    "# If you prefer symmetry, you can still wrap with identity:\n",
    "# BLEACH_all_t = PairTransformWrapper(dataset, img_tf=lambda x:x, mask_tf=lambda x:x)\n",
    "\n",
    "# --- 5) Concatenate EVERYTHING into one mega dataset ---\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "ALL_DATA = ConcatDataset([\n",
    "    BOLIVAR_t, COURTOWN_t, PAC_USA_t, IDN_PHL_t, PAC_AUS_t, TETES_t, ATL_t, TAYRONA_t,\n",
    "    CS_train_t, CS_val_t, CS_test_t,\n",
    "    BLEACH_all_t,\n",
    "])\n",
    "\n",
    "\n",
    "# --- 6) Use your existing padded collate (handles variable sizes) ---\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader_all = DataLoader(\n",
    "    ALL_DATA,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=pad_collate,   # you already defined pad_collate earlier\n",
    ")\n",
    "\n",
    "xb, yb = next(iter(loader_all))\n",
    "print(xb.shape, yb.shape)  # (B,3,Hmax,Wmax) (B,3,Hmax,Wmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc8693-e311-4b85-8182-c639cf686124",
   "metadata": {},
   "source": [
    "# YOLO Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559845c4-a241-4504-ac07-2ef04edefac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CoralTrainWrapper(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps the combined ALL_DATA dataset to produce:\n",
    "    image: (3,H,W)\n",
    "    mask: (1,H,W)\n",
    "    label: scalar (0=healthy, 1=bleached)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ds):\n",
    "        self.base = base_ds\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, mask = self.base[idx]  # from your PairTransformWrapper\n",
    "        img = img.float()\n",
    "        mask = mask.float()\n",
    "\n",
    "        # Create binary coral presence mask (1 = coral area, 0 = background)\n",
    "        coral_mask = (mask.sum(dim=0, keepdim=True) > 0).float()  # (1,H,W)\n",
    "\n",
    "        # Determine bleaching label from color mask (red>blue → bleached)\n",
    "        red_pixels = mask[0] > mask[2]\n",
    "        bleached = (red_pixels.float().mean() > 0.01)  # threshold 1% red area\n",
    "        label = torch.tensor(int(bleached), dtype=torch.long)\n",
    "\n",
    "        return {\"image\": img, \"mask\": coral_mask, \"label\": label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c7ba650-ebde-4531-a3e7-9d99b9f6c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_data = CoralTrainWrapper(ALL_DATA)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader_all = DataLoader(\n",
    "    wrapped_data,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "689d3f1e-601e-45fa-81ed-bd5fc6116a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from coral_yolo.models.coral_classifier import CoralClassifier\n",
    "from coral_yolo.losses.classification_loss import CoralClassificationLoss\n",
    "from coral_yolo.engine.metrics import ClsPRF1\n",
    "\n",
    "# --- Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CoralClassifier(\n",
    "    yolo_weights=\"yolo11n.pt\",\n",
    "    num_classes=2,\n",
    "    freeze_backbone=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = CoralClassificationLoss()\n",
    "metric = ClsPRF1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b618a9a-eb29-453a-90a1-abba5ba482d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayan\\OneDrive\\Documents\\GitHub\\Coral-reefs-DBL4\\coral_yolo\\engine\\trainer.py:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler(enabled=(self.device.type == \"cuda\"))\n",
      "Train Epoch 1/5:   0%|                                                     | 0/5532 [00:00<?, ?it/s]C:\\Users\\rayan\\OneDrive\\Documents\\GitHub\\Coral-reefs-DBL4\\coral_yolo\\engine\\trainer.py:88: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(self.device.type == \"cuda\"), dtype=torch.float16):\n",
      "                                                                                                    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcoral_yolo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[32m      3\u001b[39m trainer = Trainer(model, optimizer, device=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,grad_accum_steps=\u001b[32m4\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloader_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloader_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\GitHub\\Coral-reefs-DBL4\\coral_yolo\\engine\\trainer.py:133\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, train_loader, val_loader, epochs, verbose)\u001b[39m\n\u001b[32m    130\u001b[39m val_loader = _wrap(val_loader, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m val_loader \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     tr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m     vl = \u001b[38;5;28mself\u001b[39m._run_epoch(val_loader, train=\u001b[38;5;28;01mFalse\u001b[39;00m, epoch=ep, total_epochs=epochs) \u001b[38;5;28;01mif\u001b[39;00m val_loader \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\GitHub\\Coral-reefs-DBL4\\coral_yolo\\engine\\trainer.py:80\u001b[39m, in \u001b[36mTrainer._run_epoch\u001b[39m\u001b[34m(self, loader, train, epoch, total_epochs)\u001b[39m\n\u001b[32m     77\u001b[39m desc = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mTrain\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mtrain\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mVal\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m pbar = tqdm(loader, desc=desc, leave=\u001b[38;5;28;01mFalse\u001b[39;00m, ncols=\u001b[32m100\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmask\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mCoralTrainWrapper.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     img, mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# from your PairTransformWrapper\u001b[39;00m\n\u001b[32m     18\u001b[39m     img = img.float()\n\u001b[32m     19\u001b[39m     mask = mask.float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:346\u001b[39m, in \u001b[36mConcatDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    345\u001b[39m     sample_idx = idx - \u001b[38;5;28mself\u001b[39m.cumulative_sizes[dataset_idx - \u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mPairTransformWrapper.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     img, mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.img_tf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     39\u001b[39m         img = \u001b[38;5;28mself\u001b[39m.img_tf(img)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mSegmentationDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m     49\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMask not found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     50\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(pattern \u001b[39m\u001b[33m'\u001b[39m\u001b[33m<image_stem>_mask.png\u001b[39m\u001b[33m'\u001b[39m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     51\u001b[39m     )\n\u001b[32m     53\u001b[39m image = Image.open(img_path).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m mask = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m     57\u001b[39m     image = \u001b[38;5;28mself\u001b[39m.transform(image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\PIL\\Image.py:3513\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3512\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3513\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3514\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3515\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from coral_yolo.engine.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(model, optimizer, device=\"auto\",grad_accum_steps=4)\n",
    "trainer.fit(train_loader=loader_all, val_loader=loader_all, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6sjfypm1l1r",
   "metadata": {},
   "source": [
    "# ResNet Coral Health Classification (2 Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tbd6m6xnwf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "class CoralHealthDataset(Dataset):\n",
    "    \"\"\"\n",
    "    2-class coral health classification dataset.\n",
    "    \n",
    "    Classes:\n",
    "        0: Healthy (red channel high in mask)\n",
    "        1: Unhealthy (blue channel high in mask)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ds, transform=None):\n",
    "        self.base = base_ds\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, mask = self.base[idx]\n",
    "        \n",
    "        # img is already (3, H, W) tensor normalized to [0, 1]\n",
    "        # mask is (3, H, W) RGB tensor\n",
    "        \n",
    "        # Apply transforms\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        # Determine class from mask colors\n",
    "        # Red channel high = healthy (class 0)\n",
    "        # Blue channel high = unhealthy (class 1)\n",
    "        \n",
    "        red_mean = mask[0].mean().item()\n",
    "        blue_mean = mask[2].mean().item()\n",
    "        \n",
    "        # Classification logic\n",
    "        if red_mean > blue_mean:\n",
    "            label = 0  # Healthy (red)\n",
    "        else:\n",
    "            label = 1  # Unhealthy (blue)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "\n",
    "# Create ResNet dataset from combined data\n",
    "resnet_dataset = CoralHealthDataset(ALL_DATA)\n",
    "\n",
    "# Train/val split (80/20) - same split as YOLO\n",
    "train_size = int(0.8 * len(resnet_dataset))\n",
    "val_size = len(resnet_dataset) - train_size\n",
    "resnet_train, resnet_val = random_split(resnet_dataset, [train_size, val_size])\n",
    "\n",
    "# Create dataloaders\n",
    "resnet_train_loader = DataLoader(resnet_train, batch_size=32, shuffle=True, num_workers=0)\n",
    "resnet_val_loader = DataLoader(resnet_val, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"✓ ResNet dataset prepared:\")\n",
    "print(f\"  Train samples: {len(resnet_train)}\")\n",
    "print(f\"  Val samples: {len(resnet_val)}\")\n",
    "print(f\"  Classes: 0=Healthy, 1=Unhealthy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x1ic06hb84s",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/rayan/Desktop/Coral-reefs-DBL4')  # Update this path to your project location\n",
    "\n",
    "from resnet import CoralResNet, CoralTrainer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "resnet_model = CoralResNet(\n",
    "    num_classes=2,\n",
    "    pretrained=True,\n",
    "    freeze_backbone=True\n",
    ")\n",
    "\n",
    "# Setup training\n",
    "optimizer = optim.AdamW(resnet_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = CoralTrainer(\n",
    "    model=resnet_model,\n",
    "    device=device,\n",
    "    class_names=[\"Healthy\", \"Unhealthy\"]\n",
    ")\n",
    "\n",
    "print(\"✓ ResNet model initialized\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in resnet_model.parameters()):,}\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in resnet_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k0xk6j6qb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ResNet model\n",
    "print(\"Starting ResNet training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = trainer.fit(\n",
    "    train_loader=resnet_train_loader,\n",
    "    val_loader=resnet_val_loader,\n",
    "    epochs=10,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "print(f\"Best model saved to: resnet/best_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
