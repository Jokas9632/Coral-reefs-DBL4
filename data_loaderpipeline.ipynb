{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a4c6bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'win32com'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwin32com\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresolve_shortcut\u001b[39m(path):\n\u001b[32m      4\u001b[39m     shell = win32com.client.Dispatch(\u001b[33m\"\u001b[39m\u001b[33mWScript.Shell\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'win32com'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import win32com.client\n",
    "def resolve_shortcut(path):\n",
    "    shell = win32com.client.Dispatch(\"WScript.Shell\")\n",
    "    shortcut = shell.CreateShortCut(path)\n",
    "    return shortcut.Targetpath\n",
    "\n",
    "data_path = resolve_shortcut(r\"G:\\\\My Drive\\\\dc4data.lnk\")\n",
    "benthic_path = resolve_shortcut(data_path+r\"\\\\benthic_datasets.lnk\")\n",
    "coralbleaching_path = resolve_shortcut(data_path+r\"\\\\coral_bleaching.lnk\")\n",
    "if not os.path.exists(r\"G:\\.shortcut-targets-by-id\\1v4g4qOrbisBvrpqOxLrYn96nd_gPG_Ge\\dc4data\\coralscapes\"):\n",
    "     coralscapes_path = resolve_shortcut(data_path+r\"\\\\coralscapes.lnk\")\n",
    "else:\n",
    "        coralscapes_path = r\"G:\\.shortcut-targets-by-id\\1v4g4qOrbisBvrpqOxLrYn96nd_gPG_Ge\\dc4data\\coralscapes\"\n",
    "for p in [data_path, benthic_path, coralbleaching_path, coralscapes_path]:\n",
    "    if os.path.exists(p):\n",
    "        print(f\"Path exists: {p}\")\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Path does not exist: {p}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494b98d",
   "metadata": {},
   "source": [
    "## Benthic Datset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203fed2",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b408d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayan\\anaconda3\\envs\\dc4\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, Union, Sequence, List\n",
    "\n",
    "benthic_paths = [r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAFLOWER_BOLIVAR\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAFLOWER_COURTOWN\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAVIEW_PAC_USA\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAVIEW_IDN_PHL\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAVIEW_PAC_AUS\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\TETES_PROVIDENCIA\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\SEAVIEW_ATL\",\n",
    "r\"G:\\.shortcut-targets-by-id\\1mx2OJcVKp1mRbTbjezqWucDXpbGrd_OA\\benthic_datasets\\mask_labels\\reef_support\\UNAL_BLEACHING_TAYRONA\",]\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # keep only typical image files; sorted for reproducibility\n",
    "        exts = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\")\n",
    "        self.images = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(exts)])\n",
    "\n",
    "        if not self.images:\n",
    "            raise FileNotFoundError(f\"No images found in {img_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "        # Map \"name.ext\" -> \"name_mask.png\" (as you requested)\n",
    "        stem = Path(img_name).stem\n",
    "        mask_name = f\"{stem}_mask.png\"\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "\n",
    "        if not os.path.exists(mask_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Mask not found for {img_name}. Expected: {mask_path} \"\n",
    "                \"(pattern '<image_stem>_mask.png').\"\n",
    "            )\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "def get_mask(benthic_folder):\n",
    "    mask_path = os.path.join(benthic_folder, 'masks_stitched')\n",
    "    return mask_path\n",
    "\n",
    "def get_image(benthic_folder):\n",
    "    image_path = os.path.join(benthic_folder, 'images')\n",
    "    return image_path\n",
    "\n",
    "\n",
    "#DATASETS\n",
    "SEAFLOWER_BOLIVAR = SegmentationDataset(get_image(benthic_paths[0]), get_mask(benthic_paths[0]))\n",
    "SEAFLOWER_COURTOWN = SegmentationDataset(get_image(benthic_paths[1]), get_mask(benthic_paths[1]))\n",
    "SEAVIEW_PAC_USA = SegmentationDataset(get_image(benthic_paths[2]), get_mask(benthic_paths[2]))\n",
    "SEAVIEW_IDN_PHL = SegmentationDataset(get_image(benthic_paths[3]), get_mask(benthic_paths[3]))\n",
    "SEAVIEW_PAC_AUS = SegmentationDataset(get_image(benthic_paths[4]), get_mask(benthic_paths[4]))\n",
    "TETES_PROVIDENCIA = SegmentationDataset(get_image(benthic_paths[5]), get_mask(benthic_paths[5]))\n",
    "SEAVIEW_ATL = SegmentationDataset(get_image(benthic_paths[6]), get_mask(benthic_paths[6]))\n",
    "UNAL_BLEACHING_TAYRONA = SegmentationDataset(get_image(benthic_paths[7]), get_mask(benthic_paths[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427dad19",
   "metadata": {},
   "source": [
    "## Coral Scapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c8eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- helper: load masks from parquet parts by dataset index ---\n",
    "class _ParquetMasksByIndex:\n",
    "    def __init__(self, parquet_dir_or_paths: Union[str, Path, Sequence[Union[str, Path]]],\n",
    "                 column_png: str = \"label_health_rgb_png\"):\n",
    "        # normalize to list of parquet files\n",
    "        if isinstance(parquet_dir_or_paths, (str, Path)):\n",
    "            p = Path(parquet_dir_or_paths)\n",
    "            if p.is_dir():\n",
    "                paths = sorted(p.glob(\"*.parquet\"))\n",
    "                if not paths:\n",
    "                    raise FileNotFoundError(f\"No parquet files in directory: {p}\")\n",
    "            else:\n",
    "                if not p.exists():\n",
    "                    raise FileNotFoundError(f\"Parquet file not found: {p}\")\n",
    "                paths = [p]\n",
    "        else:\n",
    "            paths = [Path(x) for x in parquet_dir_or_paths]\n",
    "            for p in paths:\n",
    "                if not p.exists():\n",
    "                    raise FileNotFoundError(f\"Parquet file not found: {p}\")\n",
    "\n",
    "        self._tables = [pq.read_table(p) for p in paths]\n",
    "        for t in self._tables:\n",
    "            if \"index\" not in t.column_names or column_png not in t.column_names:\n",
    "                raise ValueError(f\"Parquet must have 'index' and '{column_png}'. Got: {t.column_names}\")\n",
    "        self._colname = column_png\n",
    "\n",
    "        # build index -> (table_id, row_id)\n",
    "        self._map = {}\n",
    "        for tid, t in enumerate(self._tables):\n",
    "            idxs = t[\"index\"].to_pylist()\n",
    "            for rid, ds_idx in enumerate(idxs):\n",
    "                self._map[int(ds_idx)] = (tid, rid)\n",
    "\n",
    "    def get_mask_pil(self, ds_index: int) -> Image.Image:\n",
    "        tid, rid = self._map[ds_index]\n",
    "        cell = self._tables[tid][self._colname][rid].as_py()\n",
    "        if isinstance(cell, memoryview):\n",
    "            cell = cell.tobytes()\n",
    "        elif isinstance(cell, bytearray):\n",
    "            cell = bytes(cell)\n",
    "        return Image.open(BytesIO(cell)).convert(\"RGB\")\n",
    "    \n",
    "class CoralScapesImagesMasks(Dataset):\n",
    "    \"\"\"\n",
    "    Images:\n",
    "      - Either from HF split (set split=\"train\"/\"validation\"/\"test\")\n",
    "      - Or from local Arrow shards (set arrow_paths=[...])\n",
    "    Masks:\n",
    "      - From your Parquet export (dir or list), matched by dataset index.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 parquet_dir_or_paths: Union[str, Path, Sequence[Union[str, Path]]],\n",
    "                 split: Optional[str] = None,\n",
    "                 arrow_paths: Optional[Sequence[Union[str, Path]]] = None,\n",
    "                 img_transform: Optional[Callable] = None,\n",
    "                 mask_transform: Optional[Callable] = None):\n",
    "        if (split is None) == (arrow_paths is None):\n",
    "            raise ValueError(\"Specify exactly one image source: either `split` (HF) OR `arrow_paths` (local).\")\n",
    "\n",
    "        # Image source\n",
    "        if split is not None:\n",
    "            ds_all = load_dataset(\"EPFL-ECEO/coralscapes\")\n",
    "            if split not in ds_all:\n",
    "                raise ValueError(f\"Split '{split}' not found. Available: {list(ds_all.keys())}\")\n",
    "            self.img_ds: HFDataset = ds_all[split]\n",
    "        else:\n",
    "            paths = [Path(p) for p in arrow_paths]\n",
    "            for p in paths:\n",
    "                if not p.exists():\n",
    "                    raise FileNotFoundError(f\"Arrow file not found: {p}\")\n",
    "            shards = [HFDataset.from_file(p.as_posix()) for p in paths]\n",
    "            self.img_ds = shards[0] if len(shards) == 1 else HFDataset.concatenate_datasets(shards)\n",
    "\n",
    "        # Masks\n",
    "        self.masks = _ParquetMasksByIndex(parquet_dir_or_paths)\n",
    "\n",
    "        self.img_tf = img_transform\n",
    "        self.mask_tf = mask_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ds)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        rec = self.img_ds[idx]\n",
    "        img: Image.Image = rec[\"image\"].convert(\"RGB\")\n",
    "        mask: Image.Image = self.masks.get_mask_pil(idx)  # keyed by dataset index\n",
    "\n",
    "        if self.img_tf is not None:\n",
    "            img = self.img_tf(img)\n",
    "        if self.mask_tf is not None:\n",
    "            mask = self.mask_tf(mask)\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "# --- your instantiation lines (unchanged) ---\n",
    "TRAIN_PARQUET_DIR = r\"data_preprocessing\\coralscapes_export\\parquet\\train\"\n",
    "VAL_PARQUET_DIR   = r\"data_preprocessing\\coralscapes_export\\parquet\\validation\"\n",
    "TEST_PARQUET_DIR  = r\"data_preprocessing\\coralscapes_export\\parquet\\test\"\n",
    "\n",
    "CORALSCAPES_train = CoralScapesImagesMasks(split=\"train\", parquet_dir_or_paths=TRAIN_PARQUET_DIR)\n",
    "CORALSCAPES_val   = CoralScapesImagesMasks(split=\"validation\", parquet_dir_or_paths=VAL_PARQUET_DIR)\n",
    "CORALSCAPES_test  = CoralScapesImagesMasks(split=\"test\", parquet_dir_or_paths=TEST_PARQUET_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34905e8c",
   "metadata": {},
   "source": [
    "## Coral Bleaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3dd5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_bleaching_images = r\"g:\\.shortcut-targets-by-id\\1jGkNA1n0znoxKnQBHTJZuPgvkiu_OBM8\\coral_bleaching\\reef_support\\UNAL_BLEACHING_TAYRONA\\images\"\n",
    "coral_bleaching_combined_masks = r\"data_preprocessing/coralbleaching/combined_masks\"\n",
    "coral_bleaching_single_masks = r\"data_preprocessing/coralbleaching/single_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7451e285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 640, 640]) torch.Size([8, 3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "resize = Resize((640, 640))\n",
    "\n",
    "def pil_to_tensor(img):\n",
    "    \"\"\"Converts PIL image to normalized torch tensor and resizes to 640x640.\"\"\"\n",
    "    if img is None:\n",
    "        raise ValueError(\"Received None instead of a PIL.Image.\")\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        return img  # already a tensor\n",
    "    # --- NEW: resize before converting to tensor ---\n",
    "    img = resize(img)\n",
    "    a = np.asarray(img.convert(\"RGB\"), dtype=np.uint8).copy()\n",
    "    return torch.from_numpy(a).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "\n",
    "class CoralBleachingDataset(Dataset):\n",
    "    def __init__(self, images_dir, combined_dir, single_dir):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.combined_dir = Path(combined_dir)\n",
    "        self.single_bleached = Path(single_dir) / \"bleached_blue\"\n",
    "        self.single_non = Path(single_dir) / \"non_bleached_red\"\n",
    "\n",
    "        imgs = []\n",
    "        for e in (\"*.png\",\"*.jpg\",\"*.jpeg\"):\n",
    "            imgs += list(self.images_dir.glob(e))\n",
    "        self.images = sorted(imgs)\n",
    "\n",
    "        self.pairs = self._match_pairs()\n",
    "\n",
    "    def _match_pairs(self):\n",
    "        def index_dir(d):\n",
    "            out={}\n",
    "            for e in (\"*.png\",\"*.jpg\",\"*.jpeg\"):\n",
    "                for p in d.glob(e): out[p.stem.lower()] = p\n",
    "            return out\n",
    "        cmb = index_dir(self.combined_dir)\n",
    "        ble = index_dir(self.single_bleached)\n",
    "        non = index_dir(self.single_non)\n",
    "\n",
    "        pairs=[]\n",
    "        for img in self.images:\n",
    "            key = img.stem.lower()\n",
    "            k_cmb = f\"{key}_combined\"\n",
    "            if k_cmb in cmb: pairs.append((img, cmb[k_cmb])); continue\n",
    "            cand = [p for k,p in ble.items() if k.startswith(key) or key in k]\n",
    "            if cand: pairs.append((img, cand[0])); continue\n",
    "            cand = [p for k,p in non.items() if k.startswith(key) or key in k]\n",
    "            if cand: pairs.append((img, cand[0]))\n",
    "        return pairs\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, i):\n",
    "        ip, mp = self.pairs[i]\n",
    "        x = pil_to_tensor(Image.open(ip))\n",
    "        y = pil_to_tensor(Image.open(mp))\n",
    "        return x, y  # (3,H,W), (3,H,W)\n",
    "\n",
    "def pad_collate(batch):\n",
    "    # batch: list of (img, mask) with varying H,W\n",
    "    imgs, masks = zip(*batch)\n",
    "    C = imgs[0].shape[0]\n",
    "    H = max(t.shape[1] for t in imgs)\n",
    "    W = max(t.shape[2] for t in imgs)\n",
    "    xb = torch.zeros(len(imgs), C, H, W, dtype=imgs[0].dtype)\n",
    "    yb = torch.zeros(len(masks), C, H, W, dtype=masks[0].dtype)\n",
    "    for i, (x, y) in enumerate(zip(imgs, masks)):\n",
    "        h, w = x.shape[1], x.shape[2]\n",
    "        xb[i, :, :h, :w] = x\n",
    "        yb[i, :, :h, :w] = y\n",
    "    return xb, yb\n",
    "\n",
    "# ---- use it ----\n",
    "dataset = CoralBleachingDataset(\n",
    "    images_dir=r\"g:\\.shortcut-targets-by-id\\1jGkNA1n0znoxKnQBHTJZuPgvkiu_OBM8\\coral_bleaching\\reef_support\\UNAL_BLEACHING_TAYRONA\\images\",\n",
    "    combined_dir=r\"data_preprocessing/coralbleaching/combined_masks\",\n",
    "    single_dir=r\"data_preprocessing/coralbleaching/single_masks\"\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0, collate_fn=pad_collate)\n",
    "\n",
    "xb, yb = next(iter(loader))\n",
    "print(xb.shape, yb.shape)  # -> (B,3,H_max,W_max) (B,3,H_max,W_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8315f",
   "metadata": {},
   "source": [
    "# Combine ALL into ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15160a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rayan\\AppData\\Local\\Temp\\ipykernel_41116\\128034685.py:12: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:209.)\n",
      "  return torch.from_numpy(a).permute(2,0,1).float() / 255.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 2529, 3349]) torch.Size([8, 3, 2529, 3349])\n"
     ]
    }
   ],
   "source": [
    "# --- 1) Common PIL->tensor transform for both image and mask ---\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def pil_to_tensor_rgb(img):\n",
    "    if img is None:\n",
    "        raise ValueError(\"Received None instead of a PIL.Image. Check your dataset/__getitem__.\")\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        return img  # already a tensor\n",
    "    a = np.asarray(img.convert(\"RGB\"), dtype=np.uint8)\n",
    "    return torch.from_numpy(a).permute(2,0,1).float() / 255.0\n",
    "\n",
    "\n",
    "class ToTensorPair:\n",
    "    \"\"\"Apply the same PIL->tensor conversion to (image, mask) pairs.\"\"\"\n",
    "    def __call__(self, img: Image.Image, mask: Image.Image) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return pil_to_tensor_rgb(img), pil_to_tensor_rgb(mask)\n",
    "\n",
    "# --- 2) A tiny wrapper to enforce a uniform transform across heterogeneous datasets ---\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PairTransformWrapper(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps any (image, mask) dataset and applies (img_tf, mask_tf) before returning.\n",
    "    If the underlying dataset already returns tensors, you can pass identity lambdas.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ds: Dataset, img_tf=None, mask_tf=None):\n",
    "        self.base = base_ds\n",
    "        self.img_tf = img_tf\n",
    "        self.mask_tf = mask_tf\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img, mask = self.base[idx]\n",
    "        if self.img_tf is not None:\n",
    "            img = self.img_tf(img)\n",
    "        if self.mask_tf is not None:\n",
    "            mask = self.mask_tf(mask)\n",
    "        return img, mask\n",
    "\n",
    "# --- 4) Standardize outputs: wrap PIL-returning datasets so *all* output tensors ---\n",
    "to_tensor = ToTensorPair()\n",
    "\n",
    "# Benthic sets likely return PIL unless you set transform; wrap them:\n",
    "BOLIVAR_t     = PairTransformWrapper(SEAFLOWER_BOLIVAR,     img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "COURTOWN_t    = PairTransformWrapper(SEAFLOWER_COURTOWN,    img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "PAC_USA_t     = PairTransformWrapper(SEAVIEW_PAC_USA,       img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "IDN_PHL_t     = PairTransformWrapper(SEAVIEW_IDN_PHL,       img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "PAC_AUS_t     = PairTransformWrapper(SEAVIEW_PAC_AUS,       img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "TETES_t       = PairTransformWrapper(TETES_PROVIDENCIA,     img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "ATL_t         = PairTransformWrapper(SEAVIEW_ATL,           img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "TAYRONA_t     = PairTransformWrapper(UNAL_BLEACHING_TAYRONA, img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "\n",
    "# Coralscapes (PIL) → wrap as tensors too:\n",
    "CS_train_t    = PairTransformWrapper(CORALSCAPES_train,     img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "CS_val_t      = PairTransformWrapper(CORALSCAPES_val,       img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "CS_test_t     = PairTransformWrapper(CORALSCAPES_test,      img_tf=pil_to_tensor_rgb, mask_tf=pil_to_tensor_rgb)\n",
    "\n",
    "# CoralBleachingDataset already returns tensors (3,H,W). If so, no wrap needed:\n",
    "BLEACH_all_t  = dataset  # keep as-is\n",
    "# If you prefer symmetry, you can still wrap with identity:\n",
    "# BLEACH_all_t = PairTransformWrapper(dataset, img_tf=lambda x:x, mask_tf=lambda x:x)\n",
    "\n",
    "# --- 5) Concatenate EVERYTHING into one mega dataset ---\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "ALL_DATA = ConcatDataset([\n",
    "    BOLIVAR_t, COURTOWN_t, PAC_USA_t, IDN_PHL_t, PAC_AUS_t, TETES_t, ATL_t, TAYRONA_t,\n",
    "    CS_train_t, CS_val_t, CS_test_t,\n",
    "    BLEACH_all_t,\n",
    "])\n",
    "\n",
    "\n",
    "# --- 6) Use your existing padded collate (handles variable sizes) ---\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader_all = DataLoader(\n",
    "    ALL_DATA,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=pad_collate,   # you already defined pad_collate earlier\n",
    ")\n",
    "\n",
    "xb, yb = next(iter(loader_all))\n",
    "print(xb.shape, yb.shape)  # (B,3,Hmax,Wmax) (B,3,Hmax,Wmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc8693-e311-4b85-8182-c639cf686124",
   "metadata": {},
   "source": [
    "# YOLO Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559845c4-a241-4504-ac07-2ef04edefac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CoralTrainWrapper(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps the combined ALL_DATA dataset to produce:\n",
    "    image: (3,H,W)\n",
    "    mask: (1,H,W)\n",
    "    label: scalar (0=healthy, 1=bleached)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ds):\n",
    "        self.base = base_ds\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, mask = self.base[idx]  # from your PairTransformWrapper\n",
    "        img = img.float()\n",
    "        mask = mask.float()\n",
    "\n",
    "        # Create binary coral presence mask (1 = coral area, 0 = background)\n",
    "        coral_mask = (mask.sum(dim=0, keepdim=True) > 0).float()  # (1,H,W)\n",
    "\n",
    "        # Determine bleaching label from color mask (red>blue → bleached)\n",
    "        red_pixels = mask[0] > mask[2]\n",
    "        bleached = (red_pixels.float().mean() > 0.01)  # threshold 1% red area\n",
    "        label = torch.tensor(int(bleached), dtype=torch.long)\n",
    "\n",
    "        return {\"image\": img, \"mask\": coral_mask, \"label\": label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c7ba650-ebde-4531-a3e7-9d99b9f6c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_data = CoralTrainWrapper(ALL_DATA)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader_all = DataLoader(\n",
    "    wrapped_data,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "689d3f1e-601e-45fa-81ed-bd5fc6116a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from coral_yolo.models.coral_classifier import CoralClassifier\n",
    "from coral_yolo.losses.classification_loss import CoralClassificationLoss\n",
    "from coral_yolo.engine.metrics import ClsPRF1\n",
    "\n",
    "# --- Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CoralClassifier(\n",
    "    yolo_weights=\"yolo11s.pt\",\n",
    "    num_classes=2,\n",
    "    freeze_backbone=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = CoralClassificationLoss()\n",
    "metric = ClsPRF1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b618a9a-eb29-453a-90a1-abba5ba482d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:121] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4330899456 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcoral_yolo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[32m      3\u001b[39m trainer = Trainer(model, optimizer, device=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloader_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloader_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\GitHub\\Coral-reefs-DBL4\\coral_yolo\\engine\\trainer.py:107\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, train_loader, val_loader, epochs, verbose)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     tr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     vl = \u001b[38;5;28mself\u001b[39m._run_epoch(val_loader, train=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m val_loader \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\GitHub\\Coral-reefs-DBL4\\coral_yolo\\engine\\trainer.py:63\u001b[39m, in \u001b[36mTrainer._run_epoch\u001b[39m\u001b[34m(self, loader, train)\u001b[39m\n\u001b[32m     60\u001b[39m masks  = batch[\u001b[33m\"\u001b[39m\u001b[33mmask\u001b[39m\u001b[33m\"\u001b[39m].to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     61\u001b[39m labels = batch[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(logits, labels)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\GitHub\\Coral-reefs-DBL4\\coral_yolo\\models\\coral_classifier.py:26\u001b[39m, in \u001b[36mCoralClassifier.forward\u001b[39m\u001b[34m(self, image, mask)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Runs forward pass and returns logits [B, num_classes].\"\"\"\u001b[39;00m\n\u001b[32m     25\u001b[39m feat = \u001b[38;5;28mself\u001b[39m.backbone(image)                               \u001b[38;5;66;03m# BxCxhxw\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m mfeat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmask_enc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_hw\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# BxCm×hxw\u001b[39;00m\n\u001b[32m     27\u001b[39m fused = torch.cat([feat, mfeat], dim=\u001b[32m1\u001b[39m)                   \u001b[38;5;66;03m# Bx(C+Cm)×hxw\u001b[39;00m\n\u001b[32m     28\u001b[39m fused = \u001b[38;5;28mself\u001b[39m.fuse(fused)                                  \u001b[38;5;66;03m# BxC×hxw\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\GitHub\\Coral-reefs-DBL4\\coral_yolo\\models\\mask_encoder.py:22\u001b[39m, in \u001b[36mMaskEncoder.forward\u001b[39m\u001b[34m(self, mask, feat_hw)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Downscales mask to (h,w) using nearest and runs the CNN.\"\"\"\u001b[39;00m\n\u001b[32m     21\u001b[39m m = F.interpolate(mask, size=feat_hw, mode=\u001b[33m\"\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\dc4\\Lib\\site-packages\\torch\\nn\\functional.py:2817\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2814\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2815\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2817\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2818\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2821\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2822\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at alloc_cpu.cpp:121] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4330899456 bytes."
     ]
    }
   ],
   "source": [
    "from coral_yolo.engine.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(model, optimizer, device=\"auto\")\n",
    "trainer.fit(train_loader=loader_all, val_loader=loader_all, epochs=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
